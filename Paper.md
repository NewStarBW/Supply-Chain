# 用于供应链管理中车辆路径规划的带路径记录器的多智能体强化学习方法

雷仁（IEEE 会员）、范晓阳、崔进（IEEE 会员）、沈震（IEEE 会员）、吕宜生（IEEE 高级会员）、熊刚（IEEE 高级会员）

## 摘要

在现代供应链系统中，大规模运输任务需要多辆车辆协同工作才能按时完成。过去几十年间，多车辆路径规划主要通过启发式算法实现，但这些算法面临计算时间长的难题。近年来，部分基于机器学习的方法也被用于车辆路径规划，但现有算法难以解决多车辆时间敏感型问题。为解决该问题，本文提出一种新型多智能体强化学习模型，可同时优化路径长度与车辆到达时间。该模型基于编码器 - 解码器框架：编码器挖掘问题中客户节点间的关联，解码器迭代生成每辆车的路径。特别地，本文设计多个路径记录器，用于提取车辆的路径历史信息并实现车辆间的通信。在推理阶段，该模型可针对新问题实例快速生成所有车辆的路径。为进一步提升模型性能，本文设计多采样策略，并确定计算时间与性能提升间的平衡边界。此外，本文提出一种基于仿真的车辆配置方法，用于在实际应用中选择最优车辆数量。为验证模型有效性，本文针对不同客户数量与车辆数量的问题开展一系列实验。结果表明，所提模型在性能与计算时间两方面均优于其他典型算法。

**关键词**：车辆路径规划；供应链管理；多智能体强化学习（MARL）；路径记录器

## 1 引言

物流是供应链的关键环节。在供应链管理中，以尽可能低的成本及时向下游企业供应货物，可带来显著的经济效益 \[1]。此外，“零库存” 是供应链中的核心概念，企业期望持有极少甚至零库存 \[2]。优良的路径规划方法既能降低运输成本，又能优化库存管理。实际上，几乎所有物流运输问题都可抽象为车辆路径问题（Vehicle Routing Problem, VRP）及其变体。作为典型的组合优化问题，VRP 已得到数十年的广泛研究 \[3]。

传统 VRP 的目标是找到最短路径，以满足一组具有特定需求的客户 \[4]。但在供应链场景中，最短路径仅能帮助上游企业降低运输成本；为保证下游企业生产销售计划正常推进并最小化库存成本，车辆还需按时完成运输任务。本文采用 “时间窗” 表示客户的最佳到达时段，采用 “时间窗违反惩罚” 表示车辆早到或晚到产生的成本。此外，单辆车难以满足复杂的时间窗约束与大规模客户需求，因此需多辆车协同完成运输任务，且车辆数量的选择也需纳入考量。基于上述特征，本文将供应链管理中的 VRP 抽象为 “带软时间窗的多车辆路径问题（Multi-Vehicle Routing Problem with Soft Time Windows, MVRPSTW）”\[5]。

目前已有多种启发式方法用于解决 MVRPSTW 与多智能体任务分配问题，主要分为两类：一类是智能优化算法，如遗传算法（Genetic Algorithm, GA）\[6]、蚁群算法（Ant Colony Algorithm, ACO）\[7] 等，这类算法通过迭代搜索解空间，在可接受时间内找到可行解；另一类是局部搜索算法，包括迭代局部搜索（Iterated Local Search, ILS）\[8]、禁忌搜索（Tabu Search）\[9]、大邻域搜索（Large Neighborhood Search, LNS）\[10] 等，其从初始解出发，每次从当前解的邻域解空间中选择最优邻域解。最终解的质量在很大程度上依赖于初始解的质量 \[11]。构造算法通过特定规则高效生成解，常为局部搜索算法提供初始解，例如插入算法 \[12]、Clark-Wright 节约算法 \[13]。启发式方法虽也可用于多智能体任务分配，但这类方法通常计算量大，且易陷入局部最优。

近年来，深度强化学习（Deep Reinforcement Learning, DRL）在多个领域取得显著成果 \[14,15]。得益于神经网络强大的学习能力，训练良好的 DRL 模型无需大量计算即可给出特定问题的解；此外，DRL 能够学习问题数据的分布与解空间特征，在解决时间敏感型问题方面具有巨大潜力 \[16]。

DRL 也被用于解决组合优化问题：Vinyals 等人改进序列到序列（seq2seq）模型，提出指针网络（Pointer Network）以解决旅行商问题（Traveling Salesman Problem, TSP）\[17]；Bello 等人将指针网络与 DRL 结合，利用奖励信号训练神经网络 \[18]；Nazari 等人用一维卷积替代指针网络的循环神经网络编码器，大幅降低计算复杂度 \[19]；Zhao 等人将 DRL 的输出作为局部搜索算法的初始解，进一步提升解的质量 \[20]；Kool 等人提出基于注意力机制的模型（Attention-Based Model, AM），可解决多种组合优化问题 \[21]；Lu 等人提出 “学习改进（Learn to Improve, L2I）” 方法，先随机构造可行解，再通过 DRL 对初始解进行改进 \[22]。在多车辆调度问题中，Zhang 等人将 AM 扩展到多智能体场景，用于解决 MVRPSTW \[23]；Yu 等人将指针网络应用于在线 VRP \[16]；Shi 等人提出一种 “去中心化学习 - 中心化决策” 的 DRL 框架，用于网约车服务 \[24]。

然而，在实际供应链管理场景中，仍存在以下挑战：第一，需设计有效机制实现多车辆间的通信，引导车辆协同完成配送任务；第二，算法需在优化路径长度的同时考虑时间窗违反惩罚，因此需将这两个优化目标合理融入算法框架；第三，对于特定问题实例，车辆数量过多或过少都会增加时间窗违反惩罚，实际应用中亟需一种可提前预测最优车辆数量的方法。

为此，本文提出一种带观测信息融合的新型多智能体强化学习模型，以应对上述挑战。该模型基于编码器 - 解码器框架 \[25] 迭代构造路径，特别地，为每辆车设计门控循环单元（Gated Recurrent Unit, GRU）作为 “路径记录器”，用于挖掘每辆车路径历史中的空间位置信息，并将路径记录器的输出融入每个智能体的观测空间。为实现车辆间通信，额外设计一个路径记录器以提取所有车辆的历史信息。模型经离线训练后，无需进一步训练即可为新问题实例快速生成车辆路径。此外，本文提出一种基于仿真的方法确定最优车辆数量：在任务分配前，用不同数量的车辆进行仿真分析，进而选择最适合的车辆数量执行配送任务。

本文的主要贡献如下：



1. 提出一种融合多路径记录器的多智能体强化学习模型，用于解决供应链管理中的 MVRPSTW。设计多个路径记录器构建观测空间并实现多车辆间通信，模型同时将路径长度与时间窗违反惩罚作为优化目标。

2. 所提模型可确定特定问题的最优车辆数量：通过在问题中用不同数量的车辆进行仿真分析，可同步确定最优车辆数量及对应的车辆路径。

3. 在多种场景下开展一系列实验，从成本与计算时间两方面验证模型性能；设计多采样策略进一步提升模型性能，并分析采样数量与性能提升率的关系。

本文剩余部分结构如下：第二节详细描述问题定义；第三节介绍所提模型的各组成部分；第四节通过综合案例分析评估模型性能；第五节总结全文。

## 2 问题定义

在本文的问题定义中，每个问题实例包含一组客户（数量为$N_{c}$）、一组车辆（数量为$N_{v}$）以及一个 depot（配送中心）。客户与配送中心的位置在欧几里得平面内随机生成。每个客户具有多个特征，包括位置（$v_{i}$）、需求（$d_{i}$）、服务时间（$s_{i}$）、时间窗（$e_{i}, l_{i}$），因此可表示为元组$c_{i}=(v_{i}, d_{i}, s_{i}, e_{i}, l_{i})$（$i=1,2,...,N_{c}$）。每辆车具有固定容量（$Q$）与速度（$E$）。各变量的详细定义如表 1 所示。

**表 1 变量定义**



| 变量               | 定义                                         |
| ---------------- | ------------------------------------------ |
| $v_{0}$          | 配送中心位置                                     |
| $v_{i}$          | 客户节点$i$的位置                                 |
| $d_{i}$          | 客户节点$i$的需求                                 |
| $s_{i}$          | 客户节点$i$的服务时间                               |
| $(e_{i}, l_{i})$ | 客户节点$i$的时间窗（$e_{i}$为最早到达时间，$l_{i}$为最晚到达时间） |
| $N_{c}$          | 客户总数                                       |
| $N_{v}$          | 车辆总数                                       |
| $E$              | 车辆速度                                       |
| $Q$              | 车辆最大容量                                     |
| $L_{i}$          | 车辆$i$的剩余负载                                 |
| $r_{i}$          | 车辆$i$的路径                                   |
| $t_{i,j}$        | 车辆$i$到达客户$j$的时间                            |
| $\alpha$         | 早到惩罚系数                                     |
| $\beta$          | 晚到惩罚系数                                     |

多辆同类型车辆从配送中心出发，依次满足客户需求，需遵循以下规则：



1. 车辆从配送中心出发，访问若干客户后返回配送中心；客户的访问顺序构成车辆的路径。

2. 路径以配送中心为起点和终点，配送中心仅能出现在路径两端；车辆返回配送中心后，不得再访问其他客户。

3. 车辆具有最大负载（容量$Q$），一条路径上所有客户的需求总和不得超过该容量。

4. 客户时间窗为 “软时间窗”，即车辆可在时间窗之外访问客户，但会产生惩罚；惩罚定义为到达时间与时间窗的线性函数，具体示意如图 1 所示。

5. 车辆不得在客户节点等待；到达客户节点后，需立即提供服务，服务完成后离开。

一个问题实例的解可表示为$R=(r_{1}, r_{2}, ..., r_{N_{v}})$，其中$r_{i}$为第$i$辆车的路径。问题的目标是找到成本最小的解。对于实例$P$与解$R$，成本计算如下：

$ 
\text{Length}(r_{i} \mid P) = \sum_{j=1}^{\vert r_{i} \vert - 1} \| v_{r_{i}(j)}, v_{r_{i}(j+1)} \|_{2} \tag{1}
 $

$ 
\text{Penalty}(r_{i} \mid P) = \sum_{j \in r_{i}} \left( (e_{j} - t_{i,j}) \cdot \alpha \cdot k_{e} + (t_{i,j} - l_{j}) \cdot \beta \cdot k_{l} \right) \tag{2}
 $

$ 
\text{Cost}(R \mid P) = \sum_{i=1}^{N_{v}} \left( \text{Length}(r_{i} \mid P) + \text{Penalty}(r_{i} \mid P) \right) \tag{3}
 $

其中，$\text{Length}(r_{i} \mid P)$为车辆$i$的总路径距离，$\| \cdot \|_{2}$为$\ell_{2}$范数，$r_{i}(j)$为车辆$i$路径中的第$j$个客户；$\text{Penalty}(r_{i} \mid P)$为车辆$i$的早到 / 晚到惩罚，$t_{i,j}$为车辆$i$到达客户$j$的时间；两客户间的行驶时间等于其距离除以车辆速度$E$，当前客户的到达时间等于前一客户的到达时间、前一客户的服务时间与两客户间行驶时间之和；$\alpha$、$\beta$分别为早到、晚到惩罚系数；$k_{e}$、$k_{l}$为早到、晚到标识（到达时间早于时间窗时$k_{e}=1$，晚于时间窗时$k_{l}=1$，否则为 0）。

在实际供应链系统中，客户时间窗常存在重叠或间隔极近的情况，此时单辆车难以高效满足时间窗约束（例如，两个客户时间窗极近时，单辆车仅能选择其中一个客户满足时间窗要求）。针对这一情况，本文提出多智能体深度强化学习方法，利用多辆车同时为不同客户提供服务，任务分配与路径规划过程如图 2 所示。

## 3 多智能体强化学习模型

本节介绍多智能体深度强化学习模型，该模型包含管理模块与策略模块：管理模块作为 DRL 的交互环境，策略模块由编码器、多个路径记录器与解码器组成，可根据管理模块提供的信息生成每辆车的路径。模型架构如图 3 所示。

训练过程中，管理模块生成问题实例供策略模块求解；策略模块接收实例后，先通过编码器处理客户信息，再结合编码器输出与路径记录器构建智能体状态，逐步输出车辆路径；最后根据输出解及其对应成本更新策略模块中神经网络的参数。

### 3.1 管理模块

管理模块承担 DRL 环境的角色，除生成问题实例外，还需在策略模块输出车辆下一个目的地、车队移动时更新问题状态信息，是生成可行解的关键。该模块根据特定分布生成大量训练与验证用实例，并在车队移动时更新客户与车辆的状态（状态包括每辆车的当前位置、剩余负载以及已访问的客户）。

当策略模块为某辆车指定下一个目的地后，该车辆的位置更新为所选客户的位置，剩余负载更新为$L_{t} = L_{t-1} - d_{i}$（其中$L_{t}$为当前负载，$L_{t-1}$为前一时间步的负载，$d_{i}$为所选客户$i$的需求）。受容量约束，每个时间步的$L_{t}$必须大于 0。

此外，为避免生成不可行路径，需采用掩码策略屏蔽当前时间步不可访问的客户（包括已访问的客户或需求超过车辆剩余负载的客户）\[18]。掩码策略可表示为：

$ 
M_{i,j} = 
\begin{cases} 
1, & \text{è¥å®¢æ·}i\text{å·²è¢«è®¿é®æéæ±è¶è¿è½¦è¾}j\text{çå½åè´è½½} \tag{4a} \\
0, & \text{å¶ä»æåµ}
\end{cases}
 $

$ 
M_{0,j} = 
\begin{cases} 
1, & \text{è¥è½¦è¾}j\text{ä»æå¯è®¿é®çå®¢æ·} \\
0, & \text{å¶ä»æåµ}
\end{cases} \tag{4b}
 $

其中，$M_{i,j}$为车辆$j$对客户$i$的掩码，$M_{0,j}$为车辆$j$对配送中心的掩码。

### 3.2 策略模块

策略模块接收管理模块提供的问题数据与掩码信息，将其转换为每辆车的路径序列。具体而言，通过编码器提取客户节点间的关联特征，计算每个节点的嵌入向量与整个问题的上下文嵌入向量；为每辆车配置一个 GRU 单元作为 “局部路径记录器”，提取每辆车的历史空间位置信息；额外配置一个 GRU 单元作为 “全局路径记录器”，整合所有车辆的历史信息；最后通过解码器迭代输出路径的下一个节点。

在协同多智能体问题中，所有车辆为同类型（容量与速度相同），协同完成同一目标。可认为每辆车的最优策略相同，车队的最优策略是各车辆最优策略的组合 \[24,26]。因此，本文对所有车辆采用相同的调度策略，即共享各车辆策略网络的参数 \[27]。以下详细介绍该模块的各组成部分。

#### 3.2.1 编码器

编码器用于学习问题中客户节点的表示，基于多头自注意力机制构建（与注意力模型 \[21] 类似）。给定一组客户与一个配送中心，首先通过线性投影将其映射到指定维度，得到初始嵌入向量：

$ 
h_{i}^{(0)} = w_{0} c_{i} + b_{0} \tag{5}
 $

随后利用多个注意力层计算最终嵌入向量。与 Transformer 编码器架构 \[28] 类似，每个注意力层包含两个子层：第一个子层基于多头自注意力机制，称为多头注意力层（Multi-Head Attention, MHA）；第二个子层为简单的前馈操作，称为前馈层（Feed Forward, FF）。每个子层均添加批量归一化（Batch Normalization, BN）与残差连接 \[29,30]。初始嵌入向量输入注意力层后，迭代更新得到最终嵌入向量。每个多头注意力层的计算如下：

$ 
\text{head}_{i}^{\ell} = \text{softmax}\left( \frac{Q_{i}^{\ell} (K_{i}^{\ell})^{T}}{\sqrt{d_{k}}} \right) V_{i}^{\ell}, \quad i=1,2,...,N_{h} \tag{6}
 $

$ 
\hat{h}^{(\ell)} = \text{BN}\left( \left[ \text{head}_{0}^{\ell}; \text{head}_{1}^{\ell}; ... ; \text{head}_{N_{h}}^{\ell} \right] W^{\ell} + h^{(\ell-1)} \right) \tag{7}
 $

其中，$N_{h}$为注意力头数量；$h^{(\ell-1)} = [h_{0}^{(\ell-1)}; h_{1}^{(\ell-1)}; ... ; h_{N_{c}}^{(\ell-1)}]$为初始嵌入向量或前一注意力层输出的节点嵌入向量；为简化表述，省略层索引$\ell$。在第$\ell$层中，$Q_{i} = h_{i}^{(\ell-1)} W_{i}^{Q}$、$K_{i} = h_{i}^{(\ell-1)} W_{i}^{K}$、$V_{i} = h_{i}^{(\ell-1)} W_{i}^{V}$，参数矩阵$W_{i}^{Q} \in \mathbb{R}^{d_{\text{embed}} \times d_{q}}$、$W_{i}^{K} \in \mathbb{R}^{d_{\text{embed}} \times d_{k}}$、$W_{i}^{V} \in \mathbb{R}^{d_{\text{embed}} \times d_{v}}$、$W^{\ell} \in \mathbb{R}^{N_{h} d_{v} \times d_{\text{embed}}}$。为便于计算，将注意力机制中查询（Query）、键（Key）、值（Value）的维度设置为相同，即$d_{q} = d_{k} = d_{v} = d_{\text{embed}} / N_{h}$；为使 softmax 函数输出合理数值以保证梯度有效性，将其输入乘以$1/\sqrt{d_{k}}$。

注意力层的第二个子层为带批量归一化与残差连接的前馈层：

$ 
h_{i}^{(\ell)} = \text{BN}\left( \text{FF}(\hat{h}_{i}^{(\ell)}) + \hat{h}_{i}^{(\ell)} \right) \tag{8}
 $

其中，$\text{FF}(\hat{h}_{i}^{(\ell)}) = w_{2}^{(\ell)} \text{ReLU}(w_{1}^{(\ell)} \hat{h}_{i}^{(\ell)} + b_{1}^{(\ell)}) + b_{2}^{(\ell)}$。

将初始嵌入向量输入$N_{l}$个注意力层，迭代计算式（7）与式（8）$N_{l}$次，得到每个节点的最终嵌入向量$h_{i}^{(N_{l})}$；进而计算整个问题的上下文嵌入向量$\bar{h}^{(N_{l})}$：

$ 
\bar{h}^{(N_{l})} = \frac{1}{N_{c} + 1} \sum_{i=0}^{N_{c}} h_{i}^{(N_{l})} \tag{9}
 $

#### 3.2.2 路径记录器

在协同多智能体问题中，多个学习智能体需共同寻找问题的联合解，某一智能体的行为会影响其他智能体的观测。为使模型中每个智能体能全面观测全局状态信息并实现智能体间通信，需将当前车辆与其他车辆的路径历史信息纳入状态空间。为得到历史信息的向量表示以用于后续神经网络计算，本文为每个智能体（即每辆车）配置一个 GRU 单元 \[31]：每个时间步将车辆当前位置输入 GRU 单元，得到历史信息的向量表示。每辆车的 GRU 单元称为 “局部路径记录器”，用于记录单辆车的移动历史；额外配置的 GRU 单元称为 “全局路径记录器”，用于获取所有车辆的移动历史信息。每个时间步为每辆车提供自身与其他车辆的移动历史，使其了解全局行驶状态。

具体而言，每个时间步将车辆的当前位置、剩余负载以及前一时间步局部路径记录器的隐藏状态输入局部路径记录器，输出包含该车辆移动历史信息的向量$H_{t}^{i}$。对于时间步$t$的车辆$i$，该过程可表示为：

$ 
R_{t}^{i} = \text{sigmoid}\left( a_{t-1}^{i} W_{ar}^{i} + H_{t-1}^{i} W_{hr}^{i} + b_{r}^{i} \right) \tag{10a}
 $

$ 
Z_{t}^{i} = \text{sigmoid}\left( a_{t-1}^{i} W_{az}^{i} + H_{t-1}^{i} W_{hz}^{i} + b_{z}^{i} \right) \tag{10b}
 $

$ 
\hat{H}_{t}^{i} = \tanh\left( a_{t-1}^{i} W_{ah}^{i} + (R_{t}^{i} \odot H_{t-1}^{i}) W_{hh}^{i} + b_{h}^{i} \right) \tag{10c}
 $

$ 
H_{t}^{i} = Z_{t}^{i} \odot H_{t-1}^{i} + (1 - Z_{t}^{i}) \odot \hat{H}_{t}^{i} \tag{10d}
 $

其中，$a_{t-1}^{i} \in \mathbb{R}^{3}$为车辆$i$的当前位置与剩余负载，$H_{t-1}^{i} \in \mathbb{R}^{d_{\text{embed}}}$为 GRU 单元前一时间步的隐藏状态，$\odot$表示元素 - wise 乘法。

每辆车选择下一个访问客户前，需更新局部路径记录器与全局路径记录器的隐藏状态。与局部路径记录器不同，全局路径记录器在每个时间步的输入为所有车辆的位置与负载信息。下一小节将介绍如何利用路径记录器为每辆车构建观测空间。

#### 3.2.3 解码器

解码器依次为每辆车生成路径序列。在每个解码步$t \in \{1,2,...,T\}$，解码器基于编码器的嵌入向量与路径记录器的隐藏状态，输出每辆车的下一个节点。解码过程中，为避免多辆车同时选择同一节点，采用与文献 \[23] 相同的方式，逐车生成路径的下一个节点，而非同时为所有车辆分配节点。给定问题实例$P$与解码策略参数$\theta$，时间步$t$每辆车下一个节点的计算方式如下：

$ 
\begin{aligned}
p(r_{1}^{t} \mid P, \theta) &= p(r_{1}^{t} \mid P, \theta, r_{1}^{t-1}, r_{2}^{t-1}, ..., r_{m-1}^{t-1}, r_{m}^{t-1}) \\
p(r_{2}^{t} \mid P, \theta) &= p(r_{2}^{t} \mid P, \theta, r_{1}^{t}, r_{2}^{t-1}, ..., r_{m-1}^{t-1}, r_{m}^{t-1}) \\
&\vdots \\
p(r_{m}^{t} \mid P, \theta) &= p(r_{m}^{t} \mid P, \theta, r_{1}^{t}, r_{2}^{t}, ..., r_{m-1}^{t}, r_{m}^{t-1})
\end{aligned} \tag{11}
 $

其中，$r_{i}^{t}$为时间步$t$前车辆$i$已选择的客户，$m$为车辆数量。

观测空间的构成包括：编码器计算的问题上下文嵌入向量$\bar{h}^{(N_{l})}$、局部路径记录器的隐藏状态；为使每辆车能全面观测全局状态，还需加入全局路径记录器的隐藏状态。观测空间可表示为：

$ 
o_{t}^{i} = \bar{h}^{(N_{l})} + H_{t}^{i} + \bar{H}_{t} \tag{12}
 $

其中，$o_{t}^{i}$为时间步$t$车辆$i$的观测向量，$H_{t}^{i}$为车辆$i$局部路径记录器的隐藏状态，$\bar{H}_{t}$为全局路径记录器的隐藏状态。

部分方法 \[21,23] 将车辆当前位置与负载作为状态信息，智能体仅基于当前信息决策，并通过掩码机制避免无效动作；这类方法未考虑历史动作信息，在相同位置与负载下，策略网络的输出完全相同，在多智能体且含时间窗约束的场景中效果不佳。加入路径记录器后，智能体可基于历史信息做出更优决策。

解码过程与指针网络 \[17] 类似，采用简化的注意力机制计算访问下一个客户的概率：将车辆$i$的观测向量作为查询（Query）$q_{i}^{t}$，编码阶段计算的节点嵌入向量作为键（Key）$k_{j}$\[28]，通过注意力兼容性计算车辆$i$在时间步$t$选择客户$j$的概率：

$ 
q_{i}^{t} = W_{q} o_{t}^{i} \tag{13a}
 $

$ 
k_{j} = W_{k} h_{j}^{(N_{l})} \tag{13b}
 $

$ 
u_{i,j,t} = \frac{(q_{i}^{t})^{T} k_{j}}{\sqrt{d_{k}}} \tag{13c}
 $

随后通过 softmax 函数计算选择每个客户的概率。为确保每个客户仅被访问一次且路径上客户需求总和不超过车辆最大容量，将已访问客户或需求超过车辆当前负载的客户对应的$u_{i,j,t}$设为$-\infty$\[18]；经 softmax 计算后，无效动作的概率为 0，且所有动作概率之和仍为 1。车辆$i$在时间步$t$选择客户$j$的概率为：

$ 
p_{i,j,t} = \text{softmax}(u_{i,j,t}) = \frac{e^{u_{i,j,t}}}{\sum_{j=0}^{N_{c}} e^{u_{i,j,t}}} \tag{14}
 $

此时，问题可建模为马尔可夫决策过程（Markov Decision Process, MDP），其核心组成定义如下：



* **状态（State）**：每辆车视为一个智能体，状态包含客户与车辆信息。具体而言，每个时间步车辆可获取客户的位置、需求、时间窗等信息，还可通过路径记录器获取自身与其他车辆的路径历史信息。

* **动作（Action）**：每个时间步每辆车选择一个客户提供服务；一个 episode（回合）结束后，所有智能体的联合动作构成解$R$。

* **奖励（Reward）**：将成本$\text{Cost}(R \mid P)$的负值作为奖励信号；一个 episode 结束后，根据奖励信号更新策略网络的参数。

### 3.3 训练方法

本文的问题实例无标签数据，因此无法采用监督学习训练神经网络；而是通过强化学习最小化生成路径的成本，以训练模型中所有可训练变量。给定问题实例$P$，用$\theta$表示神经网络的参数，训练目标为：

$ 
J(\theta \mid P) = \mathbb{E}_{R \sim p_{\theta}(\cdot \mid P)} \text{Cost}(R \mid P) \tag{15}
 $

神经网络中的可训练参数包括编码器、解码器与路径记录器的参数。为训练这些参数，采用经典的策略梯度方法 \[32]：通过估计期望回报的梯度，迭代改进策略。梯度可表示为：

$ 
\nabla_{\theta} J(\theta \mid P) = \mathbb{E}_{R \sim p_{\theta}(\cdot \mid P)} \left[ (\text{Cost}(R \mid P) - b(P)) \nabla_{\theta} \log p_{\theta}(R \mid P) \right] \tag{16}
 $

其中，$b(P)$为基线函数，用于估计不同问题实例的期望成本值；它可评估问题的难度，降低训练过程的方差，加快收敛速度 \[21]；$p_{\theta}(R \mid P)$为实例$P$生成解$R$的概率，与每个解码步的概率$p_{i,j,t}$相关。

对一批实例进行训练时，函数梯度可通过批次内的蒙特卡洛采样平均值计算：

$ 
\nabla_{\theta} J(\theta) = \frac{1}{B} \sum_{i=0}^{B} \left[ (\text{Cost}(R_{i} \mid P_{i}) - b(P_{i})) \nabla_{\theta} \log p_{\theta}(R_{i} \mid P_{i}) \right] \tag{17}
 $

其中，$B$为批次大小。

训练过程中，根据式（14）计算的概率随机选择每辆车的下一个节点。为降低方差，采用与策略模型结构相同的基线模型 \[33]：基线模型通过贪心策略生成路径（即选择概率最高的路径）。训练前，策略模型参数$\theta$与基线模型参数$\theta_{BL}$设为相同；训练过程中定期更新参数：每当策略模型参数更新后，计算策略模型与基线模型的结果；当两者的 t 检验置信度达到 95% 时，用$\theta$替换$\theta_{BL}$。

模型训练多轮（epoch），每轮包含多个批次的实例。第一轮训练采用指数移动平均作为 “预热基线”，计算方式如下：

$ 
b = 
\begin{cases} 
\frac{1}{B} \sum_{i=0}^{B} \text{Cost}(R_{i} \mid P_{i}), & \text{è®­ç»ç¬¬ä¸ä¸ªæ¹æ¬¡æ¶} \\
\beta b' + (1 - \beta) \frac{1}{B} \sum_{i=0}^{B} \text{Cost}(R_{i} \mid P_{i}), & \text{è®­ç»å¶ä»æ¹æ¬¡æ¶}
\end{cases} \tag{18}
 $

其中，$B$为每个批次的实例数量，$b'$为前一个批次的基线值。模型的训练流程如算法 1 所示。

**算法 1 训练流程**



| 输入 | 训练轮数$N$、每轮批次数量$B$                                                                |
| -- | -------------------------------------------------------------------------------- |
| 输出 | 策略模型参数$\theta$                                                                   |
| 1  | 初始化策略模型参数$\theta$                                                                |
| 2  | 初始化基线模型参数$\theta_{BL} = \theta$                                                  |
| 3  | 对于轮数$\text{epoch} = 1,2,...,N$：                                                  |
| 4  |     对于批次$\text{batch} = 1,2,...,B$：                                              |
| 5  |         生成训练集$M$                                                                 |
| 6  |         计算客户与配送中心的初始嵌入向量$h_{i}^{(0)}$                                            |
| 7  |         将初始嵌入向量输入编码器，得到每个节点的最终嵌入向量$h_{i}^{(N_{l})}$与问题上下文嵌入向量$\bar{h}^{(N_{l})}$ |
| 8  |         初始化车辆位置为配送中心，设置当前负载$L = Q$（车辆容量）                                         |
| 9  |         设置步数$\text{step} = 0$                                                    |
| 10 |         重复：                                                                      |
| 11 |             对于车辆$i = 1,2,...,N_{v}$：                                             |
| 12 |                 将车辆$i$的当前位置与负载输入局部路径记录器$i$，得到隐藏状态$H_{t}^{i}$                     |
| 13 |                 将所有车辆的当前位置与负载输入全局路径记录器，得到隐藏状态$\bar{H}_{t}$                       |
| 14 |                 计算查询向量$q_{t}^{i}$                                                |
| 15 |                 计算选择下一个节点的概率$p_{i,j,t}$                                          |
| 16 |                 根据$p_{i,j,t}$随机选择车辆$i$的下一个节点                                     |
| 17 |                 更新掩码与车辆状态                                                        |
| 18 |             结束循环（车辆遍历）                                                           |
| 19 |             步数$\text{step} = \text{step} + 1$                                    |
| 20 |         直到所有客户均被访问                                                               |
| 21 |         计算训练实例解的成本                                                               |
| 22 |         若$\text{epoch} = 1$：                                                     |
| 23 |             用指数移动平均计算基线成本$\text{Cost}_{BL}$                                      |
| 24 |         否则：                                                                      |
| 25 |             用基线模型生成解并计算$\text{Cost}_{BL}$                                        |
| 26 |         结束条件判断                                                                   |
| 27 |         计算训练目标函数的梯度，通过 Adam 优化器更新策略模型参数$\theta$                                  |
| 28 |         若单侧配对 t 检验（$\theta, \theta_{BL}$）< 5%：                                   |
| 29 |             更新基线模型参数$\theta_{BL} = \theta$                                       |
| 30 |         结束条件判断                                                                   |
| 31 |     结束循环（批次遍历）                                                                   |
| 32 | 结束循环（轮数遍历）                                                                       |
| 33 | 返回$\theta$                                                                       |

## 4 案例分析

为评估所提模型的性能，本文开展一系列实验，从成本与运行时间两方面与其他方法进行对比。实验涵盖不同客户数量与车辆数量的问题，训练与验证用实例根据特定分布生成。此外，设计多采样策略进一步提升模型性能，并分析采样数量、运行时间与性能提升的关系。实验在配置为 Intel (R) Xeon (R) CPU E5-2650v4（2.20GHz）、64GB 内存的服务器上进行，采用 Nvidia TITAN XP GPU 加速训练过程。

### 4.1 实验设置

#### 4.1.1 问题设置

配送中心与客户的位置在$[0,1] \times [0,1]$范围内均匀生成；每个客户的时间窗长度在 0.1\~0.2 范围内均匀随机生成；每个客户的服务时间在 0.1\~0.2 范围内均匀随机生成；早到惩罚系数$\alpha = 0.5$，考虑到晚到后果更严重，设置晚到惩罚系数$\beta = 2$。

实验针对三种问题规模：客户数量$N_{c} = 20$、50、100；车辆容量与时间窗最早到达时间根据客户数量设计；客户需求根据车辆数量随机生成。上述三个参数的详细设置如表 2 所示。

**表 2 参数设置**



| 客户数量$N_{c}$ | 20     | 50     | 100    |
| ----------- | ------ | ------ | ------ |
| 时间窗最早到达时间范围 | \[0,4] | \[0,6] | \[0,8] |
| 车辆容量$Q$     | 80     | 200    | 400    |



| 车辆数量$N_{v}$ | 2       | 3       | 4       | 5       |
| ----------- | ------- | ------- | ------- | ------- |
| 客户需求范围      | \[1,10] | \[1,15] | \[1,20] | \[1,25] |

#### 4.1.2 解码策略

与其他方法对比验证模型性能时，每个解码步采用贪心策略选择概率最大的下一个节点，得到确定性解。

为进一步提升模型性能，设计多采样策略：每个节点选择步根据节点概率随机选择下一个节点；由于解的节点序列是根据概率随机生成的，同一实例输入模型时可能生成不同解（每个解称为一个 “样本”）。对同一实例生成多组不同解，评估每个样本的成本，选择成本最小的解作为最终解。

#### 4.1.3 基线方法

选择多种基线方法从性能与计算时间两方面与所提模型对比，包括最近插入法（Nearest Insertion）、随机插入法（Random Insertion）、遗传算法（GA）以及 Google OR-Tools（开源组合优化工具，可解决带约束的多智能体问题，在相关研究中常作为基线且性能优良 \[34]）。

GA 的参数设置：种群规模 200，交叉概率 0.8，变异概率 0.1，最大迭代轮数 1000；当达到最大轮数或最优解连续 20 轮未变化时，停止迭代。Google OR-Tools 采用自动选择的初始解策略与局部搜索选项。所有基线算法均在 24 进程 CPU 上运行。

#### 4.1.4 超参数

模型的初始嵌入维度设为 128；注意力机制中查询、键、值的维度均设为 128；编码器包含 3 个注意力层，每个多头注意力层含 8 个注意力头；训练阶段共训练 100 轮，每轮随机生成 640000 个实例用于训练；批次大小设为 256；采用 Adam 优化器训练模型，学习率设为$10^{-4}$\[34]。

### 4.2 实验结果

不同客户数量与车辆数量问题的对比实验结果如表 3 所示（$N_{c}$为客户数量，$N_{v}$为车辆数量）。对所提模型与其他基线算法各测试 1000 个实例，计算平均成本与总计算时间；“Time” 列表示求解 1000 个实例的总时间（单位：秒），最优结果的差距（Gap）设为 0。

**表 3 对比实验结果**



| 方法              | $N_{c}=20, N_{v}=2$ |        |        | $N_{c}=20, N_{v}=3$ |        |        |
| --------------- | ------------------- | ------ | ------ | ------------------- | ------ | ------ |
|                 | 成本                  | 差距 (%) | 时间 (秒) | 成本                  | 差距 (%) | 时间 (秒) |
| 随机插入法           | 47.73               | 246.88 | 6.20   | 32.18               | 118.32 | 3.90   |
| 最近插入法           | 28.60               | 107.85 | 6.28   | 22.93               | 55.56  | 3.94   |
| 遗传算法            | 16.59               | 20.57  | 3570   | 15.11               | 2.51   | 4059   |
| Google OR-Tools | 16.09               | 16.93  | 17.18  | 14.92               | 1.22   | 18.99  |
| 本文模型            | 13.76               | 0.00   | 0.72   | 14.74               | 0.00   | 0.46   |

所有实例中，本文模型的成本均优于其他算法，且计算时间极短；Google OR-Tools 性能仅次于本文模型，优于 GA；插入法（随机插入、最近插入）无法在缩短路径的同时有效降低时间窗惩罚，结果远差于其他方法。

小规模问题实例中，Google OR-Tools 与 GA 的结果相近，性能接近本文模型；但随着问题规模增大，这些算法与本文模型的差距逐渐扩大，GA 的性能也逐渐劣于 Google OR-Tools。大规模问题中，所提算法与其他方法的差距显著，原因有二：一是解空间随规模增大而急剧扩张；二是本文设置了严格的时间窗约束与较大的惩罚系数，导致大规模问题中不同解的成本差异显著（例如，100 个客户、2 辆车的问题中，模型训练初期的成本约为收敛后的 30 倍）。

随着问题规模增大，本文模型的计算时间仍保持在较小值，几乎无变化；而其他方法的计算时间对问题规模敏感，尤其是 GA—— 大规模问题中，GA 求解 1000 个实例的时间已难以接受。特定问题规模下，Google OR-Tools、GA 与本文模型的计算时间不随车辆数量变化；而插入法的计算时间随车辆数量增加而减少，原因是插入法每次选择一个节点插入某辆车的路径，车辆数量越多，单辆车的路径越短，计算复杂度越低。

实验结果中存在一个有趣现象：20 个客户的实例中，本文模型用 2 辆车的成本优于 3 辆车；50 个客户的实例中，3 辆车的结果最优；100 个客户的实例中，4 辆车的结果最优。相同客户数量的实例采用相同时间窗约束，车辆数量过少难以满足时间窗约束，会产生晚到惩罚；车辆数量过多虽能缩短服务时间，但由于车辆不得在客户节点等待，需立即前往下一个客户，会产生早到惩罚。选择合适的车辆数量可在早到与晚到惩罚间取得平衡，获得更优结果。实际应用中，可采用该方法：在为车辆分配路径前，用不同数量的车辆进行仿真实验，确定最优车辆数量及对应的路径。

其他算法的成本随车辆数量增加而降低，可能是因为这些算法的解质量本身较差，增加车辆数量更容易获得较优结果。为验证这一猜想，本文绘制了模型训练过程的成本曲线（图 4）。可见，训练初期模型策略未达最优，车辆数量越多，成本越小；但随着训练推进，收敛后的成本未必随车辆数量增加而减小。

### 4.3 多采样策略的实验结果

为进一步提升模型性能，设计多采样策略，结果如表 4 所示（“Sample” 后的数字表示采样数量）。实验在单 GPU 上进行，运行时间已包含在内。可见，采用多采样策略生成多个解并选择最优解，效果优于贪心策略；与贪心策略相比，采样 100 个样本时性能提升显著，但随着采样数量增加，提升效果逐渐减弱。本文绘制了不同问题规模下性能提升与计算时间的关系曲线（图 5）。

**表 4 多采样策略实验结果**



| 策略       | $N_{c}=20, N_{v}=2$ |         |        | $N_{c}=20, N_{v}=3$ |         |        |
| -------- | ------------------- | ------- | ------ | ------------------- | ------- | ------ |
|          | 成本                  | 提升率 (%) | 时间 (秒) | 成本                  | 提升率 (%) | 时间 (秒) |
| 贪心策略     | 13.76               | 0.00    | 0.22   | 14.74               | 0.00    | 0.22   |
| 采样 100   | 12.98               | 0.78    | 53.38  | 13.99               | 0.75    | 56.64  |
| 采样 500   | 12.86               | 0.90    | 57.11  | 13.88               | 0.86    | 57.56  |
| 采样 1000  | 12.81               | 0.95    | 58.01  | 13.84               | 0.90    | 61.47  |
| 采样 2500  | 12.77               | 0.99    | 73.91  | 13.79               | 0.95    | 76.25  |
| 采样 5000  | 12.74               | 1.02    | 105.76 | 13.75               | 0.99    | 104.24 |
| 采样 10000 | 12.70               | 1.06    | 172.00 | 13.71               | 1.03    | 166.93 |

（注：表 4 还包含$N_{c}=50$、$N_{c}=100$与不同$N_{v}$的结果，结构同上，此处为简化表述省略，完整结果可参考原文。）

不同规模问题中，不同车辆数量的曲线趋势相似：计算时间达到某一阈值后，性能提升效果会变得极弱。例如，20 个客户的问题中，计算时间超过 100 秒后，曲线几乎与坐标轴平行，增加采样数量对性能提升的作用极小；这一现象在其他规模问题中同样存在。通过分析不同问题规模的结果发现，1000 个采样数量最适合该问题，可在计算时间与性能提升间取得平衡。

## 5 结论

本文提出一种新型多智能体强化学习方法，用于解决供应链管理中的车辆路径规划问题，可应对客户时间窗重叠场景，并兼顾上下游企业的整体利益。所提路径规划算法主要包含三部分：基于自注意力机制的编码器，可提取并编码每个客户的信息与整个问题的上下文信息；多个路径记录器，用于记录每辆车的历史信息，确保每辆车能全面观测全局状态并实现车辆间通信；解码器，根据编码器与路径记录器的输出生成路径。由于监督学习需要大量难以获取的带标签数据，本文采用强化学习训练神经网络；为降低训练过程的方差，采用类似自批判（self-critic）\[33] 的基线方法。此外，提出基于仿真的方法确定最优车辆数量，并设计多采样策略提升模型性能。

为评估算法性能，本文根据特定分布生成不同客户数量与车辆数量的实例，与其他算法开展对比实验。结果表明，所提算法在性能与计算时间两方面均优于其他算法，说明基于神经网络的算法能有效解决带软约束的问题；模型训练完成后，计算时间显著短于其他算法，在实际问题求解中更灵活高效。通过多采样策略的不同采样数量实验，确定了兼顾计算时间与性能提升的合适采样数量。

未来，基于机器学习的车辆路径规划研究仍需解决以下问题：本文仅研究了线性惩罚函数的情况，实际应用中可能其他类型惩罚函数更适用，因此研究其他惩罚函数具有重要意义；本文车辆数量需手动设置，仍需提出能为不同实例自动选择最优车辆数量的方法；实际场景中路径规划问题具有动态性（客户需求随时间变化），开发能解决动态场景的方法至关重要。

## 参考文献

\[1] R. J. Vokurka, R. R. Lummus. The role of just-in-time in supply chain management\[J]. International Journal of Logistics Management, 2000, 11(1): 89-98.

\[2] R. W. Hall, R. A. Hall. Zero Inventories\[M]. Burr Ridge, IL, USA: Irwin Professional, 1983.

\[3] G. Kim, Y. S. Ong, C. K. Heng, et al. City vehicle routing problem (City VRP): A review\[J]. IEEE Transactions on Intelligent Transportation Systems, 2015, 16(4): 1654-1666.

\[4] P. Toth, D. Vigo. The Vehicle Routing Problem\[M]. Philadelphia, PA, USA: Society for Industrial and Applied Mathematics, 2002.

\[5] H. C. Lau, M. Sim, K. M. Teo. Vehicle routing problem with time windows and a limited number of vehicles\[J]. European Journal of Operational Research, 2003, 148(3): 559-569.

\[6] A. Király, J. Abonyi. Redesign of the supply of mobile mechanics based on a novel genetic optimization algorithm using Google maps API\[J]. Engineering Applications of Artificial Intelligence, 2015, 38: 122-130.

\[7] M. Dorigo, V. Maniezzo, A. Colorni. Ant system: Optimization by a colony of cooperating agents\[J]. IEEE Transactions on Systems, Man, and Cybernetics - Part B: Cybernetics, 1996, 26(1): 29-41.

\[8] T. Ibaraki, S. Imahori, K. Nonobe, et al. An iterated local search algorithm for the vehicle routing problem with convex time penalty functions\[J]. Discrete Applied Mathematics, 2008, 156(11): 2050-2069.

\[9] A. Lim, F. Wang. A smoothed dynamic tabu search embedded GRASP for m-VRPTW\[C]. In Proceedings of the International Conference on Tools with Artificial Intelligence, 2004: 704-708.

\[10] P. Shaw. Using constraint programming and local search methods to solve vehicle routing problems\[C]. In Proceedings of the International Conference on Principles and Practice of Constraint Programming, 1998: 417-431.

\[11] M. Zirour. Vehicle routing problem: Models and solutions\[J]. Journal of Quality Measurement and Analysis, 2008, 4(1): 205-218.

\[12] S. Joshi, S. Kaur. Nearest neighbor insertion algorithm for solving capacitated vehicle routing problem\[C]. In Proceedings of the International Conference on Computing for Sustainable Global Development, 2015: 86-88.

\[13] G. Clarke, J. W. Wright. Scheduling of vehicles from a central depot to a number of delivery points\[J]. Operations Research, 1964, 12(4): 568-581.

\[14] D. Silver, et al. Mastering the game of Go with deep neural networks and tree search\[J]. Nature, 2016, 529(7587): 484-489.

\[15] V. Mnih, et al. Human-level control through deep reinforcement learning\[J]. Nature, 2015, 518: 529-533.

\[16] J. J. Q. Yu, W. Yu, J. Gu. Online vehicle routing with neural combinatorial optimization and deep reinforcement learning\[J]. IEEE Transactions on Intelligent Transportation Systems, 2019, 20(10): 3806-3817.

\[17] O. Vinyals, M. Fortunato, N. Jaitly. Pointer networks\[EB/OL]. 2015, arXiv:1506.03134.

\[18] I. Bello, H. Pham, Q. V. Le, et al. Neural combinatorial optimization with reinforcement learning\[EB/OL]. 2016, arXiv:1611.09940.

\[19] M. Nazari, A. Oroojlooy, L. V. Snyder, et al. Reinforcement learning for solving the vehicle routing problem\[EB/OL]. 2018, arXiv:1802.04240.

\[20] J. Zhao, M. Mao, X. Zhao, et al. A hybrid of deep reinforcement learning and local search for the vehicle routing problems\[J]. IEEE Transactions on Intelligent Transportation Systems, 2020, 22(11): 7208-7218.

\[21] W. Kool, H. van Hoof, M. Welling. Attention, learn to solve routing problems\[EB/OL]. 2018, arXiv:1803.08475.

\[22] H. Lu, X. Zhang, S. Yang. A learning-based iterative method for solving vehicle routing problems\[C]. In Proceedings of the International Conference on Learning Representations, 2019: 1-15.

\[23] K. Zhang, F. He, Z. Zhang, et al. Multi-vehicle routing problems with soft time windows: A multi-agent reinforcement learning approach\[J]. Transportation Research Part C: Emerging Technologies, 2020, 121: 102861.

\[24] J. Shi, Y. Gao, W. Wang, et al. Operating electric vehicle fleet for ride-hailing services with reinforcement learning\[J]. IEEE Transactions on Intelligent Transportation Systems, 2020, 21(11): 4822-4834.

\[25] I. Sutskever, O. Vinyals, Q. V. Le. Sequence to sequence learning with neural networks\[EB/OL]. 2014, arXiv:1409.3215.

\[26] J. K. Gupta, M. Egorov, M. Kochenderfer. Cooperative multi-agent control using deep reinforcement learning\[C]. In Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, 2017: 66-83.

\[27] J. N. Foerster, Y. M. Assael, N. de Freitas, et al. Learning to communicate with deep multi-agent reinforcement learning\[EB/OL]. 2016, arXiv:1605.06676.

\[28] A. Vaswani, et al. Attention is all you need\[EB/OL]. 2017, arXiv:1706.03762.

\[29] S. Ioffe, C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift\[C]. In Proceedings of the International Conference on Machine Learning, 2015: 448-456.

\[30] K. He, X. Zhang, S. Ren, et al. Deep residual learning for image recognition\[C]. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016: 770-778.

\[31] K. Cho, et al. Learning phrase representations using RNN encoder-decoder for statistical machine translation\[EB/OL]. 2014, arXiv:1406.1078.

\[32] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning\[J]. Machine Learning, 1992, 8(3-4): 229-256.

\[33] S. J. Rennie, E. Marcheret, Y. Mroueh, et al. Self-critical sequence training for image captioning\[C]. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017: 7008-7024.

\[34] D. P. Kingma, J. Ba. Adam: A method for stochastic optimization\[EB/OL]. 2014, arXiv:1412.6980.

## 作者简介

雷仁（IEEE 会员）：2002 年、2005 年分别获山东大学学士、硕士学位，2009 年获中国科学院博士学位。现任北京航空航天大学自动化科学与电气工程学院教授、云制造研究中心副主任，兼任教育部复杂产品先进制造系统工程研究中心高级研究员。研究方向为工业大数据、工业人工智能、云制造。

范晓阳：现为北京航空航天大学自动化科学与电气工程学院硕士研究生。研究方向为工业智能、深度强化学习、组合优化。

崔进（IEEE 会员）：2013 年获太原理工大学学士学位，2019 年获北京航空航天大学博士学位。现任北京航空航天大学前沿科学研究院助理教授。研究方向为智能制造、数字孪生、不确定性量化、故障预测与健康管理。

沈震（IEEE 会员）：2004 年、2009 年分别获清华大学学士、博士学位；2007 年 10 月至 2008 年 4 月任美国波士顿大学制造工程系与信息系统工程中心访问学者。2009 年起任中国科学院自动化研究所复杂系统管理与控制国家重点实验室助理研究员，2012 年起任该实验室副研究员。研究方向为智能制造、3D 打印、复杂系统性能评估与优化、序优化。

吕宜生（IEEE 高级会员）：2005 年、2007 年分别获哈尔滨工业大学学士、硕士学位，2010 年获中国科学院博士学位。现任中国科学院自动化研究所复杂系统管理与控制国家重点实验室副研究员，兼任青岛智能产业技术研究院研究员。研究方向为交通数据分析、深度学习、机器学习、平行交通管理与控制。

熊刚（IEEE 高级会员）：1991 年、1994 年分别获西安科技大学学士、硕士学位，1996 年获上海交通大学博士学位。1996 年至 1998 年任浙江大学博士后研究员、副研究员；1998 年至 2001 年任芬兰坦佩雷理工大学高级研究员；2001 年至 2007 年任诺基亚公司（芬兰）专家、项目经理；2007 年任美国埃森哲公司、雪佛龙公司高级顾问、团队负责人；2008 年任中国科学院信息化办公室副主任；2009 年起任中国科学院自动化研究所复杂系统管理与控制国家重点实验室研究员；2011 年起任中国科学院云计算中心副主任。研究方向为平行控制与管理、复杂系统建模与优化、云计算与大数据、智能制造、智能交通系统。

（注：原文中图表（图 1 - 图 5）及完整的表 4 数据因排版限制未完全呈现，如需参考可查阅 IEEE Xplore 原文，文献编号：10.1109/TITS.2022.3150151）

如果需要，我可以帮你整理一份**论文核心创新点与实验结论的精简对照表**，方便快速抓取关键信息，你是否需要？

> （注：文档部分内容可能由 AI 生成）